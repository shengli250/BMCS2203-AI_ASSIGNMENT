{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be13bb10-fbb9-4a99-8051-d07a4297eddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Resources downloaded successfully!\n",
      "                           text              intent\n",
      "0               Zero tolerance?  ask_smoking_policy\n",
      "1            Is the hotel open?    ask_availability\n",
      "2  Are there fines for smoking?  ask_smoking_policy\n",
      "3            Cancel reservation    ask_cancellation\n",
      "4            Traveling with dog      ask_pet_policy\n",
      "--- Preprocessing Complete (with NLTK Lemmatization) ---\n",
      "                           text        cleaned_text\n",
      "0               Zero tolerance?      zero tolerance\n",
      "1            Is the hotel open?          hotel open\n",
      "2  Are there fines for smoking?        fine smoking\n",
      "3            Cancel reservation  cancel reservation\n",
      "4            Traveling with dog       traveling dog\n",
      "Feature matrix X shape: (2001, 974)\n",
      "Labels y shape: (2001,)\n",
      "Train set size:1600 samples\n",
      "Test set size:401 samples\n",
      "--- SVM Model Evaluation ---\n",
      "Accuracy: 0.773067331670823\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "  ask_airport_transfer       0.77      0.85      0.81        20\n",
      "      ask_availability       0.73      0.80      0.76        20\n",
      "           ask_booking       0.79      0.95      0.86        20\n",
      " ask_breakfast_details       0.93      0.70      0.80        20\n",
      "      ask_cancellation       0.95      0.95      0.95        20\n",
      "      ask_checkin_time       0.86      0.60      0.71        20\n",
      "     ask_checkout_time       0.68      0.95      0.79        20\n",
      "      ask_child_policy       0.93      0.67      0.78        21\n",
      "   ask_contact_support       0.89      0.80      0.84        20\n",
      "        ask_facilities       0.76      0.65      0.70        20\n",
      "          ask_location       0.55      0.60      0.57        20\n",
      "         ask_lost_item       0.70      0.70      0.70        20\n",
      "   ask_luggage_storage       0.70      0.70      0.70        20\n",
      "ask_nearby_attractions       0.44      0.85      0.58        20\n",
      "   ask_payment_methods       0.84      0.80      0.82        20\n",
      "        ask_pet_policy       1.00      0.90      0.95        20\n",
      "        ask_room_price       0.86      0.95      0.90        20\n",
      "    ask_smoking_policy       0.89      0.85      0.87        20\n",
      "               goodbye       0.80      0.40      0.53        20\n",
      "              greeting       0.94      0.80      0.86        20\n",
      "\n",
      "              accuracy                           0.77       401\n",
      "             macro avg       0.80      0.77      0.77       401\n",
      "          weighted avg       0.80      0.77      0.77       401\n",
      "\n",
      "Model and Vectorizer saved using joblib.\n",
      "\n",
      "--- Loading Responses from JSON ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mresponse.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         responses = \u001b[43mjson\u001b[49m.load(f)\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(responses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m intent-response pairs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# install necessary libraries (assuming the initial block has been run)\n",
    "# !pip install scikit-learn\n",
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "\n",
    "# download necessary nltk resources (assuming the initial block has been run)\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "print(\"NLTK Resources downloaded successfully!\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ğŸŒŸ æ›¿æ¢ï¼šå¯¼å…¥æ”¯æŒå‘é‡æœº (SVM)\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from joblib import dump,load\n",
    "\n",
    "# Import NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Placeholder: Load the actual dataset. Ensure it has 'text' (user query) and 'intent' (label) columns\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "\n",
    "# shuffle the data for robust splitting\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(df.head())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Convert to Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove Punctuation and Special Characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Stopword Removal\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Lemmatization (Key Enhancement)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Rejoin tokens into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the new preprocessing function to the text column\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "print(\"--- Preprocessing Complete (with NLTK Lemmatization) ---\")\n",
    "print(df[['text', 'cleaned_text']].head())\n",
    "\n",
    "# Prepare the cleaned text and intents for the model training section\n",
    "X = df['cleaned_text']\n",
    "y = df['intent']\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data to create the feature matrix X\n",
    "X = vectorizer.fit_transform(X)\n",
    "y = df['intent']\n",
    "\n",
    "print(f\"Feature matrix X shape: {X.shape}\")\n",
    "print(f\"Labels y shape: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y if len(df['intent'].unique()) > 1 else None, # ä»…åœ¨æœ‰å¤šä¸ªç±»åˆ«æ—¶åˆ†å±‚\n",
    ")\n",
    "\n",
    "print(f\"Train set size:{X_train.shape[0]} samples\")\n",
    "print(f\"Test set size:{X_test.shape[0]} samples\")\n",
    "\n",
    "# ğŸŒŸ æ›¿æ¢ï¼šå®ä¾‹åŒ–æ”¯æŒå‘é‡æœº (SVM) æ¨¡å‹\n",
    "# ä½¿ç”¨ 'linear' æ ¸é€šå¸¸åœ¨æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°è‰¯å¥½ï¼Œå› ä¸ºå®ƒåœ¨é«˜ç»´ç©ºé—´ä¸­è¡¨ç°é«˜æ•ˆ\n",
    "svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred = svm_model.predict(X_test)\n",
    "\n",
    "print(\"--- SVM Model Evaluation ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œåˆ†ç±»æŠ¥å‘Š\n",
    "if len(y_test.unique()) > 1:\n",
    "    print(classification_report(y_test, pred, zero_division=0))\n",
    "else:\n",
    "    print(\"Classification Report skipped: Only one class in test set.\")\n",
    "\n",
    "# ğŸŒŸ æ›¿æ¢ï¼šä¿å­˜è®­ç»ƒå¥½çš„ SVM æ¨¡å‹å’Œ Vectorizer\n",
    "dump(svm_model, 'svm_intent_model.joblib')\n",
    "dump(vectorizer, 'tfidf_vectorizer_SVM.joblib')\n",
    "print(\"Model and Vectorizer saved using joblib.\")\n",
    "\n",
    "# Predefined fixed responses (Retrieval System)\n",
    "print(\"\\n--- Loading Responses from JSON ---\")\n",
    "try:\n",
    "    with open('response.json', 'r', encoding='utf-8') as f:\n",
    "        responses = json.load(f)\n",
    "    print(f\"Successfully loaded {len(responses)} intent-response pairs.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: response.json file not found.\")\n",
    "    responses = {}\n",
    "\n",
    "# ğŸŒŸ æ›¿æ¢ï¼šæ›´æ–°å‡½æ•°å\n",
    "def chatbot_reply_svm(user_input, model, vectorizer, responses):\n",
    "    # 1. Preprocessing\n",
    "    user_input = user_input.lower()\n",
    "\n",
    "    # 2. Feature Extraction: Transform the input using the fitted vectorizer\n",
    "    # âš ï¸ æ³¨æ„: å°½ç®¡è¿™é‡Œåªè¿›è¡Œäº†å°å†™ï¼Œä½†å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œåº”è¯¥ä½¿ç”¨å®Œæ•´çš„ `preprocess_text` å‡½æ•°\n",
    "    # ä¸ºäº†ä¸ä¹‹å‰çš„ä»£ç ä¿æŒä¸€è‡´ï¼Œè¿™é‡Œä½¿ç”¨ç®€æ´ç‰ˆæœ¬ã€‚\n",
    "    user_input_cleaned = preprocess_text(user_input)\n",
    "    \n",
    "    vector = vectorizer.transform([user_input_cleaned])\n",
    "\n",
    "    # 3. Intent Prediction\n",
    "    intent = model.predict(vector)[0]\n",
    "\n",
    "    # 4. Retrieval (Check for unknown intent/fallback)\n",
    "    # If the predicted intent exists in the dictionary, return the specific response\n",
    "    # Otherwise, return a fallback message\n",
    "    return responses.get(intent, f\"Sorry, I predicted the intent '{intent}', but I don't have a specific response for that yet. Please rephrase your question.\")\n",
    "\n",
    "# Test the chatbot function\n",
    "print(\"\\n --- SVM Chatbot Test ---\")\n",
    "test_input = \"What is your best price?\"\n",
    "predicted_response = chatbot_reply_svm(test_input, svm_model, vectorizer, responses)\n",
    "print(f\"User Input: {test_input}\")\n",
    "print(f\"Chatbot Reply: {predicted_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82544f-dcec-401b-8777-a88eb7af4d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
